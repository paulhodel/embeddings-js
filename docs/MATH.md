1. Formal CSS Model
1.1 Vocabulary and frequency grid

Let the vocabulary be

ğ‘‰
=
{
ğ‘¤
1
,
â€¦
,
ğ‘¤
ğ‘‰
}
V={w
1
	â€‹

,â€¦,w
V
	â€‹

}

Fix a global semantic frequency grid of size 
ğ‘
N:

Î©
=
{
ğœ”
1
,
â€¦
,
ğœ”
ğ‘
}
Î©={Ï‰
1
	â€‹

,â€¦,Ï‰
N
	â€‹

}

Think of 
ğœ”
ğ‘—
Ï‰
j
	â€‹

 as basis semantic modes shared by all words.

1.2 Word representation: sparse spectrum

Each word 
ğ‘¤
âˆˆ
ğ‘‰
wâˆˆV has a complex spectral embedding:

ğ‘†
ğ‘¤
âˆˆ
ğ¶
ğ‘
S
w
	â€‹

âˆˆC
N

Written component-wise:

ğ‘†
ğ‘¤
=
(
ğ‘†
ğ‘¤
(
ğœ”
1
)
,
â€¦
,
ğ‘†
ğ‘¤
(
ğœ”
ğ‘
)
)
,
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
=
ğ´
ğ‘¤
(
ğœ”
ğ‘—
)
â€‰
ğ‘’
ğ‘–
ğœ™
ğ‘¤
(
ğœ”
ğ‘—
)
S
w
	â€‹

=(S
w
	â€‹

(Ï‰
1
	â€‹

),â€¦,S
w
	â€‹

(Ï‰
N
	â€‹

)),S
w
	â€‹

(Ï‰
j
	â€‹

)=A
w
	â€‹

(Ï‰
j
	â€‹

)e
iÏ•
w
	â€‹

(Ï‰
j
	â€‹

)

where

ğ´
ğ‘¤
(
ğœ”
ğ‘—
)
â‰¥
0
A
w
	â€‹

(Ï‰
j
	â€‹

)â‰¥0 is the amplitude (strength of semantic mode 
ğœ”
ğ‘—
Ï‰
j
	â€‹

),

ğœ™
ğ‘¤
(
ğœ”
ğ‘—
)
âˆˆ
[
âˆ’
ğœ‹
,
ğœ‹
)
Ï•
w
	â€‹

(Ï‰
j
	â€‹

)âˆˆ[âˆ’Ï€,Ï€) is the phase (relational orientation).

Sparsity constraint:
Each word uses at most 
ğ¾
â‰ª
ğ‘
Kâ‰ªN active frequencies.

Formally, define:

supp
(
ğ‘†
ğ‘¤
)
=
{
ğœ”
ğ‘—
âˆˆ
Î©
âˆ£
âˆ£
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
âˆ£
>
0
}
supp(S
w
	â€‹

)={Ï‰
j
	â€‹

âˆˆÎ©âˆ£âˆ£S
w
	â€‹

(Ï‰
j
	â€‹

)âˆ£>0}

with

âˆ£
supp
(
ğ‘†
ğ‘¤
)
âˆ£
â‰¤
ğ¾
âˆ£supp(S
w
	â€‹

)âˆ£â‰¤K

This is the key: few frequencies per word.

1.3 Context representation

Let a context 
ğ¶
C be a multiset (or sequence) of words:

ğ¶
=
{
ğ‘
1
,
â€¦
,
ğ‘
ğ‘š
}
,
ğ‘
ğ‘–
âˆˆ
ğ‘‰
C={c
1
	â€‹

,â€¦,c
m
	â€‹

},c
i
	â€‹

âˆˆV

We define a context spectrum 
ğ‘†
ğ¶
âˆˆ
ğ¶
ğ‘
S
C
	â€‹

âˆˆC
N
 by some aggregation function 
ğ‘”
g:

ğ‘†
ğ¶
=
ğ‘”
(
ğ‘†
ğ‘
1
,
â€¦
,
ğ‘†
ğ‘
ğ‘š
)
S
C
	â€‹

=g(S
c
1
	â€‹

	â€‹

,â€¦,S
c
m
	â€‹

	â€‹

)

The simplest choice (for clarity) is a weighted sum:

ğ‘†
ğ¶
(
ğœ”
ğ‘—
)
=
âˆ‘
ğ‘–
=
1
ğ‘š
ğ›¼
ğ‘–
â€‰
ğ‘†
ğ‘
ğ‘–
(
ğœ”
ğ‘—
)
S
C
	â€‹

(Ï‰
j
	â€‹

)=
i=1
âˆ‘
m
	â€‹

Î±
i
	â€‹

S
c
i
	â€‹

	â€‹

(Ï‰
j
	â€‹

)

where 
ğ›¼
ğ‘–
Î±
i
	â€‹

 could encode position, distance, or other context weights (e.g., closer words get larger 
ğ›¼
ğ‘–
Î±
i
	â€‹

).

More general forms are possible, but this is enough to formalize CSS.

1.4 Measurement viewpoint

CSSâ€™s core idea:
Context acts as a measurement of a wordâ€™s spectrum.

Define a compatibility score between word 
ğ‘¤
w and context 
ğ¶
C as:

score
(
ğ‘¤
,
ğ¶
)
=
â„œ
(
âˆ‘
ğ‘—
=
1
ğ‘
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
âˆ—
â€‰
ğ‘†
ğ¶
(
ğœ”
ğ‘—
)
)
=
â„œ
(
âŸ¨
ğ‘†
ğ‘¤
,
ğ‘†
ğ¶
âŸ©
ğ¶
ğ‘
)
score(w,C)=â„œ(
j=1
âˆ‘
N
	â€‹

S
w
	â€‹

(Ï‰
j
	â€‹

)
âˆ—
S
C
	â€‹

(Ï‰
j
	â€‹

))=â„œ(âŸ¨S
w
	â€‹

,S
C
	â€‹

âŸ©
C
N
	â€‹

)

where 
âˆ—
âˆ—
 is complex conjugate.

Interpretation:

The context spectrum 
ğ‘†
ğ¶
S
C
	â€‹

 is a measurement pattern over frequencies.

The inner product measures how well the wordâ€™s spectrum resonates with that measurement.

Each occurrence of 
ğ‘¤
w in a context 
ğ¶
ğ‘¡
C
t
	â€‹

 gives one such scalar 
score
(
ğ‘¤
,
ğ¶
ğ‘¡
)
score(w,C
t
	â€‹

) that we try to make high versus negatives.

1.5 Training objective (high level)

Given corpus 
ğ·
D as wordâ€“context pairs 
(
ğ‘¤
,
ğ¶
)
(w,C):

Positive set:

ğ‘ƒ
=
{
(
ğ‘¤
,
ğ¶
)
âˆ£
ğ‘¤
 appears in context 
ğ¶
}
P={(w,C)âˆ£w appears in context C}

Negative samples:
For each positive pair 
(
ğ‘¤
,
ğ¶
)
(w,C), draw negative words 
ğ‘›
n from a noise distribution 
ğ‘ƒ
neg
(
ğ‘›
)
P
neg
	â€‹

(n).

Define a contrastive loss (spectral skip-gram style):

ğ¿
data
=
âˆ’
âˆ‘
(
ğ‘¤
,
ğ¶
)
âˆˆ
ğ‘ƒ
[
log
â¡
ğœ
(
score
(
ğ‘¤
,
ğ¶
)
)
+
âˆ‘
ğ‘›
âˆ¼
ğ‘ƒ
neg
log
â¡
ğœ
(
âˆ’
score
(
ğ‘›
,
ğ¶
)
)
]
L
data
	â€‹

=âˆ’
(w,C)âˆˆP
âˆ‘
	â€‹

	â€‹

logÏƒ(score(w,C))+
nâˆ¼P
neg
	â€‹

âˆ‘
	â€‹

logÏƒ(âˆ’score(n,C))
	â€‹


with 
ğœ
Ïƒ the sigmoid.

Add sparsity and norm regularization:

ğ¿
sparsity
=
ğœ†
1
âˆ‘
ğ‘¤
âˆˆ
ğ‘‰
âˆ‘
ğ‘—
=
1
ğ‘
âˆ£
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
âˆ£
L
sparsity
	â€‹

=Î»
1
	â€‹

wâˆˆV
âˆ‘
	â€‹

j=1
âˆ‘
N
	â€‹

âˆ£S
w
	â€‹

(Ï‰
j
	â€‹

)âˆ£
ğ¿
norm
=
ğœ†
2
âˆ‘
ğ‘¤
âˆˆ
ğ‘‰
(
âˆ¥
ğ‘†
ğ‘¤
âˆ¥
2
2
âˆ’
ğ‘
)
2
L
norm
	â€‹

=Î»
2
	â€‹

wâˆˆV
âˆ‘
	â€‹

(âˆ¥S
w
	â€‹

âˆ¥
2
2
	â€‹

âˆ’c)
2

Total loss:

ğ¿
=
ğ¿
data
+
ğ¿
sparsity
+
ğ¿
norm
L=L
data
	â€‹

+L
sparsity
	â€‹

+L
norm
	â€‹


This is the Compressive Semantic Spectroscopy perspective:

The data term enforces that spectra explain the observed contexts.

The sparsity term enforces few frequencies per word.

The norm term controls total spectral power per word.

In a more â€œcompressive sensingâ€ framing, for each word 
ğ‘¤
w, all its contexts 
{
ğ¶
ğ‘¡
}
ğ‘¡
=
1
ğ‘‡
ğ‘¤
{C
t
	â€‹

}
t=1
T
w
	â€‹

	â€‹

 induce many measurement equations that jointly constrain the same sparse vector 
ğ‘†
ğ‘¤
S
w
	â€‹

.

2. Core Operations in CSS
2.1 Word similarity

Given two words 
ğ‘¤
,
ğ‘¢
w,u:

sim
(
ğ‘¤
,
ğ‘¢
)
=
â„œ
(
âŸ¨
ğ‘†
ğ‘¤
,
ğ‘†
ğ‘¢
âŸ©
âˆ¥
ğ‘†
ğ‘¤
âˆ¥
â€‰
âˆ¥
ğ‘†
ğ‘¢
âˆ¥
)
sim(w,u)=â„œ(
âˆ¥S
w
	â€‹

âˆ¥âˆ¥S
u
	â€‹

âˆ¥
âŸ¨S
w
	â€‹

,S
u
	â€‹

âŸ©
	â€‹

)

This is a complex cosine similarity (taking the real part), sensitive to both amplitudes and phases.

If we ignore phases (or use magnitude spectrum):

sim
mag
(
ğ‘¤
,
ğ‘¢
)
=
âˆ‘
ğ‘—
ğ´
ğ‘¤
(
ğœ”
ğ‘—
)
ğ´
ğ‘¢
(
ğœ”
ğ‘—
)
âˆ¥
ğ´
ğ‘¤
âˆ¥
2
â€‰
âˆ¥
ğ´
ğ‘¢
âˆ¥
2
sim
mag
	â€‹

(w,u)=
âˆ¥A
w
	â€‹

âˆ¥
2
	â€‹

âˆ¥A
u
	â€‹

âˆ¥
2
	â€‹

âˆ‘
j
	â€‹

A
w
	â€‹

(Ï‰
j
	â€‹

)A
u
	â€‹

(Ï‰
j
	â€‹

)
	â€‹

2.2 Composition (phrase/sentence meaning)

A phrase/sentence can be represented as the context spectrum:

ğ‘†
phrase
=
ğ‘”
(
ğ‘†
ğ‘¤
1
,
â€¦
,
ğ‘†
ğ‘¤
ğ‘š
)
S
phrase
	â€‹

=g(S
w
1
	â€‹

	â€‹

,â€¦,S
w
m
	â€‹

	â€‹

)

Using sum as before:

ğ‘†
phrase
(
ğœ”
ğ‘—
)
=
âˆ‘
ğ‘–
=
1
ğ‘š
ğ›¼
ğ‘–
ğ‘†
ğ‘¤
ğ‘–
(
ğœ”
ğ‘—
)
S
phrase
	â€‹

(Ï‰
j
	â€‹

)=
i=1
âˆ‘
m
	â€‹

Î±
i
	â€‹

S
w
i
	â€‹

	â€‹

(Ï‰
j
	â€‹

)

Interpretation:

Frequencies shared by multiple words are amplified (constructive interference).

Incompatible phases can cause partial cancellation (destructive interference).

This acts like a spectral blend of meanings.

2.3 Contextualization of a single word

Given a target word 
ğ‘¤
w and context 
ğ¶
C:

Define the context-filtered spectrum:

ğ‘†
~
ğ‘¤
(
ğ¶
)
(
ğœ”
ğ‘—
)
=
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
â‹…
â„
(
ğ‘†
ğ¶
(
ğœ”
ğ‘—
)
)
S
~
w
(C)
	â€‹

(Ï‰
j
	â€‹

)=S
w
	â€‹

(Ï‰
j
	â€‹

)â‹…h(S
C
	â€‹

(Ï‰
j
	â€‹

))

where 
â„
h is some filtering function, e.g.:

multiplicative filter: 
â„
(
ğ‘§
)
=
ğ‘§
h(z)=z

or normalized gating: 
â„
(
ğ‘§
)
=
ğœ
(
ğ›½
âˆ£
ğ‘§
âˆ£
)
h(z)=Ïƒ(Î²âˆ£zâˆ£) acting on amplitude

This expresses:

Context selects and scales which frequencies of the word are active.

2.4 Analogy & relations via phase shifts

If a relation 
ğ‘…
R corresponds to a phase shift pattern 
Î”
ğ‘…
(
ğœ”
ğ‘—
)
Î”
R
	â€‹

(Ï‰
j
	â€‹

):

ğ‘†
ğ‘¤
:
ğµ
(
ğ‘…
)
(
ğœ”
ğ‘—
)
=
ğ‘†
ğ‘¤
(
ğœ”
ğ‘—
)
â‹…
ğ‘’
ğ‘–
Î”
ğ‘…
(
ğœ”
ğ‘—
)
S
w:B
(R)
	â€‹

(Ï‰
j
	â€‹

)=S
w
	â€‹

(Ï‰
j
	â€‹

)â‹…e
iÎ”
R
	â€‹

(Ï‰
j
	â€‹

)

Then an analogy like:

ğ‘¤
1
:
ğ‘¤
2
:
:
ğ‘¤
3
:
?
w
1
	â€‹

:w
2
	â€‹

::w
3
	â€‹

:?

would try to find 
ğ‘¤
4
w
4
	â€‹

 such that:

ğ‘†
ğ‘¤
2
â‰ˆ
ğ‘†
ğ‘¤
1
(
ğ‘…
)
and
ğ‘†
ğ‘¤
4
â‰ˆ
ğ‘†
ğ‘¤
3
(
ğ‘…
)
S
w
2
	â€‹

	â€‹

â‰ˆS
w
1
	â€‹

(R)
	â€‹

andS
w
4
	â€‹

	â€‹

â‰ˆS
w
3
	â€‹

(R)
	â€‹


with 
ğ‘…
R inferred from the phase difference between 
ğ‘†
ğ‘¤
1
S
w
1
	â€‹

	â€‹

 and 
ğ‘†
ğ‘¤
2
S
w
2
	â€‹

	â€‹

.

This is more speculative but fits nicely in the complex representation.

3. Parallels with Other Embeddings
3.1 Classical dense embeddings (word2vec, GloVe)

Representations:

ğ‘£
ğ‘¤
âˆˆ
ğ‘…
ğ‘‘
v
w
	â€‹

âˆˆR
d

Learned by:

skip-gram / CBOW (predict context),

or factorizing co-occurrence matrices.

Parallel:

CSSâ€™s spectral vectors 
ğ‘†
ğ‘¤
âˆˆ
ğ¶
ğ‘
S
w
	â€‹

âˆˆC
N
 play the role of 
ğ‘£
ğ‘¤
v
w
	â€‹

,

but with structure:

complex-valued,

sparsity,

frequency semantics.

Key differences:

Dense vectors are typically fully dense, uninterpreted axes.

CSS is explicitly sparse, with axes interpreted as semantic frequencies.

Polysemy is implicit in dense embeddings; in CSS, it is explicitly multi-modal (multi-peak spectra).

3.2 Complex embeddings (e.g., ComplEx for knowledge graphs)

ComplEx represents entities/relations as complex vectors and scores triples by complex inner products.

Parallel:

CSS also uses complex vectors and complex inner products.

Phases encode relational structure.

Difference:

ComplEx is for triples (entity, relation, entity) in KGs.

CSS is for word meaning and contexts in distributional corpora.

CSS adds sparsity and frequency interpretation on top.

3.3 Graph-based embeddings (e.g., DeepWalk, node2vec, LINE)

These methods:

Build a graph (nodes = words, edges = co-occurrence or relations),

Learn embeddings by random walks / edge sampling / matrix factorization.

Parallel:

CSS can be seen as learning embeddings from an implicit semantic graph (words + co-occurrence edges).

The data term in 
ğ¿
data
L
data
	â€‹

 is similar to sampling edges and non-edges.

Difference:

Graph embeddings are typically real, dense vectors.

CSS uses complex, sparse spectral codes with a measurement interpretation.

CSS focuses on recovering sparse spectra rather than arbitrary dense vectors.

3.4 Contextual LMs (ELMo, BERT, GPT)

These models:

Map each token occurrence to a context-dependent embedding.

Meaning is fully entangled with context through deep networks.

Parallel:

CSS also yields contextualized spectra via context filters: 
ğ‘†
~
ğ‘¤
(
ğ¶
)
S
~
w
(C)
	â€‹

.

Both aim to model how meaning changes with context.

Difference:

LMs use large parametric neural networks and multi-layer attention.

CSS seeks a lower-level semantic representation where:

base word representations are sparse spectra,

contextualization is mostly spectral filtering and interference, not dozens of nonlinear layers.

CSS is more like a structured core semantics layer, onto which bigger models could be built.

4. Compute & Space Costs (Rough Comparison)

Let:

ğ‘‰
V = vocabulary size

ğ‘‘
d = dimension of standard embeddings

ğ‘
N = total number of semantic frequencies in CSS

ğ¾
K = max active frequencies per word in CSS (sparsity; 
ğ¾
â‰ª
ğ‘
Kâ‰ªN)

4.1 Space complexity

Dense embeddings (word2vec/GloVe):

Each word: 
ğ‘‘
d floats

Total:

ğ‘‚
(
ğ‘‰
ğ‘‘
)
O(Vd)

For example: 
ğ‘‰
=
100
ğ‘˜
,
ğ‘‘
=
300
â‡’
30
ğ‘€
V=100k,d=300â‡’30M parameters.

CSS (sparse complex spectra):

Each word: at most 
ğ¾
K non-zero complex coefficients
each coefficient = 2 real numbers (
Re
Re, 
Im
Im) + an index.

Ignoring index storage overhead:

Per word: 
âˆ¼
2
ğ¾
âˆ¼2K real values

Total:

ğ‘‚
(
ğ‘‰
ğ¾
)
O(VK)

If 
ğ¾
â‰ª
ğ‘‘
Kâ‰ªd, you can be more memory-efficient than dense embeddings.

Example:

ğ‘‰
=
100
ğ‘˜
V=100k

ğ‘
=
1024
N=1024 frequencies

ğ¾
=
16
K=16 active frequencies/word
â†’ params â‰ˆ 
100
ğ‘˜
Ã—
16
Ã—
2
=
3.2
ğ‘€
100kÃ—16Ã—2=3.2M real values
vs 30M in a 300-d dense model.

Even if you add overhead for indices, you can still be competitive.

4.2 Per-update compute (training)

Assume a skip-gram-like update with:

center word 
ğ‘¤
w,

one context 
ğ¶
C,

ğ‘š
m context words in 
ğ¶
C,

ğ‘˜
k negative samples.

Dense embeddings:

Each update uses dot products of size 
ğ‘‘
d:
cost â‰ˆ 
ğ‘‚
(
ğ‘‘
(
ğ‘š
+
ğ‘˜
)
)
O(d(m+k)).

CSS (sparse):

Key point: all spectra are sparse with at most 
ğ¾
K active frequencies. So:

The context spectrum 
ğ‘†
ğ¶
S
C
	â€‹

 has at most 
â‰¤
ğ‘š
ğ¾
â‰¤mK active entries (in practice often much fewer due to overlapping supports).

The word spectrum 
ğ‘†
ğ‘¤
S
w
	â€‹

 has at most 
ğ¾
K entries.

Computing score(w, C):

Only frequencies in 
supp
(
ğ‘†
ğ‘¤
)
âˆ©
supp
(
ğ‘†
ğ¶
)
supp(S
w
	â€‹

)âˆ©supp(S
C
	â€‹

) matter.

Worst case (no overlap): naive: 
ğ‘‚
(
ğ¾
â‹…
ğ‘š
ğ¾
)
O(Kâ‹…mK), but youâ€™d implement this with hash / index intersection â†’ cost â‰ˆ 
ğ‘‚
(
âˆ£
supp
(
ğ‘†
ğ‘¤
)
âˆ©
supp
(
ğ‘†
ğ¶
)
âˆ£
)
O(âˆ£supp(S
w
	â€‹

)âˆ©supp(S
C
	â€‹

)âˆ£). Typically much less than 
ğ¾
â‹…
ğ‘š
Kâ‹…m.

Realistically:

If frequencies are shared and structured, per-update cost:

ğ‘‚
(
ğ¾
(
ğ‘š
+
ğ‘˜
)
)
O(K(m+k))

with 
ğ¾
â‰ª
ğ‘‘
Kâ‰ªd.

So compute can be significantly cheaper than dense embeddings for similar quality, especially if you keep 
ğ¾
K small and caches of sparse indices efficient.

4.3 Overheads

CSS adds:

sparsity enforcement:

top-K pruning or L1 gradient steps (cheap per word),

potential extra bookkeeping (indices of active frequencies).

But saves:

full dense operations on dimension 
ğ‘‘
=
ğ‘
d=N,

model size.

Overall, you trade some bookkeeping and model simplicity for:

smaller effective dimensionality per word (K),

potential interpretability,

explicit multi-sense representation.

5. Short Summary

Representation:
CSS replaces dense vectors with sparse complex spectra over shared semantic frequencies.

Training:
Words are learned as sparse spectra that best explain many contextual â€œmeasurementsâ€, using a contrastive objective plus sparsity constraints. This is analogous to compressive sensing.

Operations:

Similarity = complex spectral cosine.

Composition = spectral summation/interference.

Contextualization = frequency-wise filtering.

Analogies = phase shifts across frequencies.

Parallel to existing methods:
CSS is structurally related to word2vec, graph embeddings, and complex embeddings, but adds:

sparsity,

explicit frequencies,

measurement-based perspective,

natural polysemy through multi-peak spectra.

Costs:

Space â‰ˆ 
ğ‘‚
(
ğ‘‰
ğ¾
)
O(VK) vs 
ğ‘‚
(
ğ‘‰
ğ‘‘
)
O(Vd).

Compute per update â‰ˆ 
ğ‘‚
(
ğ¾
(
ğ‘š
+
ğ‘˜
)
)
O(K(m+k)) vs 
ğ‘‚
(
ğ‘‘
(
ğ‘š
+
ğ‘˜
)
)
O(d(m+k)).

With 
ğ¾
â‰ª
ğ‘‘
Kâ‰ªd, CSS can be lighter and more interpretable.

If youâ€™d like, next we can:

formalize a concrete CSS training algorithm step-by-step (still math-level, not code),

or define a minimal experimental setup (tiny vocab + toy corpus) to illustrate how spectra and polysemy actually look.