ABSTRACT — Compressive Semantic Spectra (CSS)
A sparse spectral representation for lexical meaning
Compressive Semantic Spectra (CSS) is a framework for representing word meaning as a sparse set of semantic frequencies rather than as a single dense vector. Each component frequency corresponds to a latent semantic dimension shared across the vocabulary, where any amplitude define intensitity. This structure yields representations that are compact, interpretable, and naturally capable of expressing polysemy.
CSS treats linguistic context as a measurement of a word’s underlying semantic spectrum. Each occurrence of a word provides a partial and noisy observation, and learning consists of integrating these observations across many contexts.

In CSS, each word settles into a small number of stable frequency components that reflect the different ways it is used. Words with multiple senses tend to accumulate several distinct components, while more specific words usually retain only one or two. Contexts play a central role in shaping these patterns: when a word appears alongside others that activate similar frequencies, those components strengthen; when a component is rarely supported by the contexts in which the word occurs, it gradually fades. In this way, the model separates the competing uses of a word and assigns each to its own part of the spectrum.
Training relies on straightforward local adjustments. Each time a word appears in a context, its current spectrum is compared with the spectrum suggested by its neighbors, and the amplitudes are nudged toward or away from that local signal. Weak components that never gain consistent support are removed over time, keeping the representation compact. The method can start from random initial spectra or from the coarser structure provided by an existing embedding model.
Taken together, CSS provides a small and transparent representation of word meaning. By treating each context as one more observation of how a word is used, and by allowing only a few components to persist, the model captures both stable meaning and variation across senses without relying on heavy architectures or dense vector spaces.